{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import itertools\n",
    "import time\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(elm, size):\n",
    "    vec = np.zeros(size)\n",
    "    try:\n",
    "        vec[int(elm)] = 1\n",
    "    except: \n",
    "        vec[0] =1\n",
    "    return list(vec)\n",
    "    \n",
    "\n",
    "def merchant_average(merchant_list,merchant_dict):\n",
    "    \"\"\"\n",
    "    Calculate the average of all MERCHANT domain embeddings\n",
    "    \"\"\"\n",
    "    merchant_av = []\n",
    "    for merchant in merchant_list:\n",
    "        try:\n",
    "            merchant_av.append(merchant_dict[merchant])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return merchant_av\n",
    "\n",
    "def get_embedding(key, id_embedding, merchant_prior):\n",
    "    \"\"\"\n",
    "    Look up item embedding, if not present, take the average of all merchant domains\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embed = id_embedding[key]\n",
    "        return list(embed)\n",
    "    except:\n",
    "        embed = merchant_prior\n",
    "        return list(embed)\n",
    "\n",
    "def average_keywords_domains(kw_list, kw_embed):\n",
    "    \"\"\"\n",
    "    Get the average of all keywords used for the case when no keywords exist.\n",
    "    \"\"\"\n",
    "    domain_av = []\n",
    "    for elm in kw_list:\n",
    "        try:\n",
    "            this_word = kw_embed[elm] \n",
    "            domain_av.append(this_word)\n",
    "        except:\n",
    "            pass\n",
    "    if len(domain_av) == 0:\n",
    "        return [0]*300\n",
    "    domain_av = np.mean(np.asarray(domain_av),axis = 0)\n",
    "    return list(domain_av)\n",
    "\n",
    "def average_keywords_url(this_id, keywords, kw_dict, av_kw):\n",
    "    \"\"\" \n",
    "    Return the average keywords of domain in session, if no keywords, take average of all keywords for domain\n",
    "    \"\"\"\n",
    "    d = []\n",
    "    for elm in keywords:\n",
    "        try:\n",
    "            word = kw_dict[elm]\n",
    "            d.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    if len(d) == 0:\n",
    "        try: \n",
    "            d.append(av_kw_dict[this_id])\n",
    "            return d\n",
    "        except:\n",
    "            return [0]*300\n",
    "        \n",
    "    d = np.asarray(d)\n",
    "    d = np.reshape(np.mean(d,axis = 0), (-1,1))\n",
    "    return(list(d))\n",
    "\n",
    "\n",
    "# generate one batch per on the fly per itereation\n",
    "def batch_generation(sequences, embed_dict, batch_size, max_length, embed_size, KEY_WORDS, RANDOM):\n",
    "    \"\"\"\" Take a batch and return embedding in correct shape to be fed into RNN,\n",
    "          new_batch: shape for input placeholder with last domain in seq left for prediction\n",
    "          target: the prediction that is compared to.\n",
    "    \"\"\"\n",
    "\n",
    "    new_batch, targets, target_merchs, target_pubs, target_merch_labels, target_labels, target_indexes,future_context = [], [], [], [], [], [], [], []\n",
    "    merchant_count = 0\n",
    "    history, history_no = [], []\n",
    "    for i in range(len(sequences)):\n",
    "        this_sequence = sequences[i]\n",
    "        \n",
    "        history_tmp = []\n",
    "        if RANDOM == True:\n",
    "            rand = np.arange(len(this_sequence))\n",
    "            np.random.shuffle(rand)\n",
    "            this_sequence = [this_sequence[x] for x in rand]\n",
    "        history_tmp = []\n",
    "        history_tmp.append(this_sequence[0][0])\n",
    "        for elm in range(len(this_sequence)-1):\n",
    "\n",
    "            target_index = mapping[str(this_sequence[elm+1][0])]\n",
    "            target = np.zeros((len(ids),1)) # create a zero vector for one hot encoding\n",
    "            target_labels.append(float(target_index))\n",
    "            target[target_index] = 1 # fill in index with one\n",
    "            targets = targets+[target] # apend to the targets list\n",
    "            if this_sequence[elm+1][-1] == 1:\n",
    "                target_merchs.append(merchant_count)\n",
    "            else: target_pubs.append(merchant_count)\n",
    "\n",
    "\n",
    "            if this_sequence[elm+1][0] in history_tmp:\n",
    "                history.append(merchant_count)\n",
    "            else: history_no.append(merchant_count)\n",
    "            history_tmp.append(this_sequence[elm+1][0])\n",
    "            merchant_count+=1\n",
    "\n",
    "        # get the indeces for the lengths: many to many\n",
    "        target_lens = np.arange(len(this_sequence)-1) + i*max_length\n",
    "        target_indexes.extend(target_lens)\n",
    "            \n",
    "        future_context = future_context+[average_keywords_url(str(x[0]), x[1], kw, av_kw_domain_embed)+ one_hot(x[2],size_dt) + one_hot(x[3],size_hour) + one_hot(x[4], size_day) + one_hot(x[5],size_id) for x in this_sequence[1:]]\n",
    "        if KEY_WORDS == True:\n",
    "            new_batch = new_batch+[get_embedding([str(x[0])], embed_dict, prior)  + average_keywords_url(str(x[0]), x[1], kw, av_kw_domain_embed)for x in this_sequence[:-1]]\n",
    "    \n",
    "            \n",
    "        else: new_batch = new_batch+[get_embedding([str(x[0])], embed_dict, prior) for x in this_sequence[:-1]]\n",
    "        if (len(this_sequence[:-1])< max_length):\n",
    "            new_batch = new_batch+[[0]*embed_size for x in range((max_length+1 - len(this_sequence)))]\n",
    "\n",
    "    return np.asarray(new_batch).reshape(len(sequences), max_length, embed_size), np.asarray(targets).reshape(-1,len(ids)), np.reshape(target_labels,(-1,1)), np.asarray(target_indexes).reshape(-1,1), np.asarray(future_context).reshape(-1,embed_future), np.asarray(target_merchs).reshape(-1,1), np.asarray(target_pubs).reshape(-1,1), np.asarray(history).reshape(-1,1), np.asarray(history_no).reshape(-1,1)\n",
    "\n",
    "\n",
    "# get the correct indexes of non zero paddings.\n",
    "def get_relevant(rnn_output, indx):\n",
    "    \"\"\"\n",
    "    rnn_output: output from GRU.\n",
    "    indx: indexes of items to gathered.\n",
    "    \"\"\"\n",
    "    out_size = int(rnn_output.get_shape()[2])\n",
    "    flat = tf.reshape(rnn_output, [-1, out_size])\n",
    "    relevant = tf.gather(flat, indx)\n",
    "    return tf.reshape(relevant,[-1,out_size])    \n",
    "    \n",
    "\n",
    "# calculate the length of each sequence before paddding\n",
    "def lengths(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    used.get_shape()\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length,used # returns the length of each sequence in a batch    \n",
    "\n",
    "def optimize(data, embed_dictionary, batch_size, num_epochs, max_length, embed_size, lr, TEST_DATA):\n",
    "    num_batches = int(len(data)//batch_size)\n",
    "    num_batches_test = int(len(TEST_DATA)//batch_size)\n",
    "    epoch_acc = []\n",
    "    epoch_recall, epoch_mrr, epoch_m, epoch_p, epoch_h, epoch_nh, epoch_lens  = [], [], [], [], [], [],[]\n",
    "    costs = []\n",
    "    test_counter = 0\n",
    "    stop_count = 0\n",
    "    recall_list, mrr_list = [], []\n",
    "    test_freq = int(num_batches//10)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "\n",
    "        acc_list=[]\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())        \n",
    "        start_epoch = time.time()\n",
    "        # For each epoch loop over all batches and optimize the cost and produce the test cost\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"-------Running Epoch:{}-------\".format(epoch+1))\n",
    "            epoch_loss = 0\n",
    "            np.random.shuffle(data)\n",
    "            start = time.time()\n",
    "            freq = int(num_batches//40)\n",
    "            batch_time = 0\n",
    "            len_recall = np.zeros((max_length,1))\n",
    "            len_counts = np.zeros((max_length,1))\n",
    "            for i in range(num_batches):\n",
    "                count = 0\n",
    "                recall = 0\n",
    "\n",
    "                batch_data = data[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "\n",
    "                # Get the batches ready and into the correct form and shape                \n",
    "                epoch_x, epoch_y, label_ind, out_ind, fut_context, m_ind, p_ind, h_ind, nh_ind = batch_generation(batch_data, embed_dictionary, batch_size, max_length, embed_size, KEY_WORDS,   RANDOM = False)\n",
    "                feed_dict={seq: epoch_x, targets: epoch_y, train_labels:label_ind, num_samples: n_samples, output_indexes: out_ind, context_future:fut_context, merchant_labels : m_ind, publisher_labels : p_ind, history_labels: h_ind, no_history_labels: nh_ind}\n",
    "                _, c, acc, this_recall, mrr, m_recall, p_recall, h_recall, nh_recall, r_hits, summary = sess.run([optimizer, loss, accuracy, final_recall, recip_rank, M_final_recall, P_final_recall,  H_final_recall, NH_final_recall, recall_hits,merged_summary_op], feed_dict)              \n",
    "                r_hits = np.reshape(r_hits, (-1,1))\n",
    "                #summary_writer.add_summary(summary, epoch * num_batches + i)\n",
    "                for j in range(max_length):\n",
    "                    if j in out_ind % 19:\n",
    "                        len_indx = out_ind%19 == j\n",
    "                        len_counts[j] += 1\n",
    "                        len_indx = r_hits[len_indx]\n",
    "                        len_recall[j] += np.mean(len_indx)\n",
    "                recall_list.append(this_recall)        \n",
    "                epoch_loss += c\n",
    "                mrr_list.append(mrr)\n",
    "                costs.append(c)\n",
    "                acc_list.append(acc)\n",
    "                summary_writer.add_summary(summary, epoch * num_batches + i)\n",
    "                if (i)% (freq) ==0 or i == (num_batches):\n",
    "                    saver.save(sess= sess, save_path = save_model) \n",
    "                    print(\"-------Running group:{} out of :{}-------\".format(i+1, num_batches))\n",
    "                    print(\"Trained {} batches with current batch cost: {} and accuracy: {}, recall {}\".format(i+1,c, acc,this_recall))\n",
    "                    print(\"Trained {} batches with average accuracy: {}, average cost {}, average recall{}\".format(i+1,np.sum(acc_list)/len(acc_list),np.sum(costs)/len(acc_list), np.sum(recall_list)/len(acc_list)))\n",
    "                    print(\"Current run time: {} \".format(time.time()-start_epoch))\n",
    "                    print(\"Time per {} batches: {}\\n \".format(freq, time.time()-batch_time))\n",
    "                    batch_time = time.time()\n",
    "                if i% test_freq == 0 or i == num_batches:\n",
    "                    np.random.shuffle(TEST_DATA)\n",
    "                    batch_time  =time.time()\n",
    "                    batch_data = TEST_DATA[:batch_size]\n",
    "                    epoch_x, epoch_y, label_ind, out_ind, fut_context, m_ind, p_ind, h_ind, nh_ind = batch_generation(batch_data, embed_dictionary, batch_size, max_length, embed_size, KEY_WORDS,   RANDOM = False)\n",
    "                    feed_dict={seq: epoch_x, targets: epoch_y, train_labels:label_ind, num_samples: n_samples, output_indexes: out_ind, context_future:fut_context, merchant_labels : m_ind, publisher_labels : p_ind, history_labels: h_ind, no_history_labels: nh_ind}\n",
    "                    acc,  this_recall, mrr, m_recall, p_recall, h_recall, nh_recall = sess.run([  accuracy,final_recall, recip_rank, M_final_recall, P_final_recall, H_final_recall, NH_final_recall ], feed_dict)\n",
    "                    print(\"##############################################\")\n",
    "                    print(\"-------Running Test after:{} out of :{}-------\".format(i+1, num_batches))\n",
    "                    print(\"Trained {} batches with current batch accuracy: {}, recall {}, mrr {} \".format(i+1, acc,this_recall, mrr))\n",
    "                    print(\"Current run time: {} \".format(time.time()-start_epoch))\n",
    "                    print(\"Time per batch: {}\\n \".format( time.time()-batch_time))\n",
    "                    print(\"##############################################\\n\")\n",
    "                    test_counter+=1\n",
    "            recall_by_len_train = np.divide(len_recall, len_counts)\n",
    "            where_are_NaNs = np.isnan(recall_by_len_train)\n",
    "            recall_by_len_train[where_are_NaNs] = 0\n",
    "            print(recall_by_len_train)\n",
    "            len_recall = np.zeros((max_length,1))\n",
    "            len_counts = np.zeros((max_length,1))\n",
    "        \n",
    "            acc_test,recall_test, R_acc_test, R_recall_test = [], [], [], []\n",
    "            mrr_test, R_mrr_test = [], []\n",
    "            mer_recall, pub_recall = [], []\n",
    "            R_mer_recall, R_pub_recall = [], []\n",
    "            h_recall = 0\n",
    "            nh_recall = 0\n",
    "            R_h_recall = 0\n",
    "            R_nh_recall = 0\n",
    "            for i in range(num_batches_test):\n",
    "                batch_data = TEST_DATA[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "\n",
    "                # Get the batches ready and into the correct form and shape                \n",
    "                epoch_x, epoch_y, label_ind, out_ind, fut_context, m_ind, p_ind, h_ind, nh_ind = batch_generation(batch_data, embed_dictionary, batch_size, max_length, embed_size, KEY_WORDS,   RANDOM = False)\n",
    "                feed_dict={seq: epoch_x, targets: epoch_y, train_labels:label_ind, num_samples: n_samples, output_indexes: out_ind, context_future:fut_context, merchant_labels : m_ind, publisher_labels : p_ind, history_labels: h_ind, no_history_labels: nh_ind}\n",
    "                acc,  this_recall, mrr, m_recall, p_recall, h_r, nh_r, r_hits = sess.run([  accuracy,final_recall, recip_rank, M_final_recall, P_final_recall, H_final_recall, NH_final_recall, recall_hits ], feed_dict)\n",
    "                r_hits = np.reshape(r_hits, (-1,1))\n",
    "                for j in range(max_length):\n",
    "                    if j in out_ind % 19:\n",
    "                        len_indx = out_ind%19 == j\n",
    "                        len_counts[j] += 1\n",
    "                        len_indx = r_hits[len_indx]\n",
    "                        len_recall[j] += np.mean(len_indx)\n",
    "                        \n",
    "                acc_test.append(acc)\n",
    "                recall_test.append(this_recall)\n",
    "                mrr_test.append(mrr)\n",
    "                mer_recall.append(m_recall)\n",
    "                pub_recall.append(p_recall)\n",
    "                h_recall+= h_r\n",
    "                nh_recall += nh_r\n",
    "                \n",
    "                epoch_x, epoch_y, label_ind, out_ind, fut_context, m_ind, p_ind, h_ind, nh_ind = batch_generation(batch_data, embed_dictionary, batch_size, max_length, embed_size, KEY_WORDS,   RANDOM = True)\n",
    "                feed_dict={seq: epoch_x, targets: epoch_y, train_labels:label_ind, num_samples: n_samples, output_indexes: out_ind, context_future:fut_context, merchant_labels : m_ind, publisher_labels : p_ind, history_labels: h_ind, no_history_labels: nh_ind}\n",
    "                R_acc,  R_this_recall, R_mrr, R_m_recall, R_p_recall, R_h_r, R_nh_r =  sess.run([  accuracy,final_recall, recip_rank, M_final_recall, P_final_recall, H_final_recall, NH_final_recall ], feed_dict)\n",
    "                R_acc_test.append(R_acc)\n",
    "                R_recall_test.append(R_this_recall)\n",
    "                R_mrr_test.append(R_mrr)\n",
    "                R_mer_recall.append(R_m_recall)\n",
    "                R_pub_recall.append(R_p_recall)\n",
    "                R_h_recall+= R_h_r\n",
    "                R_nh_recall += R_nh_r\n",
    "            recall_by_len_test = np.divide(len_recall, len_counts)\n",
    "            where_are_NaNs = np.isnan(recall_by_len_test)\n",
    "            recall_by_len_test[where_are_NaNs] = 0                \n",
    "            print(\"Test recall: {}, Test accuracy: {}, merchant recall: {}, publisher recall {}, mrr {}, history {}, no history{}\".format(np.mean(recall_test),np.mean(acc_test), np.mean(mer_recall), np.mean(pub_recall), np.mean(mrr_test), h_recall/(i+1), nh_recall/(i+1)))\n",
    "            print(\"RANDOM: Test recall: {}, Test accuracy: {},  merchant recall: {}, publisher recall {}, mrr {}, history {}, no history{}\".format(np.mean(R_recall_test),np.mean(R_acc_test), np.mean(R_mer_recall), np.mean(R_pub_recall), np.mean(R_mrr_test), R_h_recall/(i+1), R_nh_recall/(i+1)))\n",
    "            print(recall_by_len_test)\n",
    "            print(\"Total time taken for epoch : {:f}\".format(time.time()-start_epoch)) \n",
    "            epoch_acc.append(np.mean(acc_test))\n",
    "            epoch_recall.append(np.mean(recall_test)) \n",
    "            epoch_mrr.append(np.mean(mrr_test))\n",
    "            epoch_m.append(np.mean(mer_recall))\n",
    "            epoch_p.append(np.mean(pub_recall))\n",
    "            epoch_h.append(np.mean(h_recall))\n",
    "            epoch_nh.append(np.mean(nh_recall))\n",
    "            epoch_lens.append(recall_by_len_test)\n",
    "            if epoch_acc[epoch] - epoch_recall[epoch-1]<0:\n",
    "                lr = lr/2\n",
    "                stop_count+=1\n",
    "                if stop_count == 4:\n",
    "                    \n",
    "                    return  epoch_acc, epoch_recall, epoch_mrr, epoch_m, epoch_p, epoch_h, epoch_nh\n",
    "    return  epoch_acc, epoch_recall, epoch_mrr, epoch_m, epoch_p, epoch_h, epoch_nh\n",
    "\n",
    "def reommend_pub_merchants(data, embed_dictionary, batch_size, max_length, embed_size, recall_num):   \n",
    "    num_batches = int(len(data)//batch_size)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver2restore = tf.train.Saver()\n",
    "        saver2restore.restore(sess = sess, save_path= save_model)\n",
    "        p_rec_list,m_rec_list = [], []                       \n",
    "        for i in range(num_batches):\n",
    "            batch_data = data[i*batch_size:(i+1)*batch_size]\n",
    "            epoch_x, epoch_y, label_ind, out_ind, fut_context, m_ind, p_ind, h_ind, nh_ind = batch_generation(batch_data, embed_dictionary, batch_size, max_length, embed_size, KEY_WORDS,   RANDOM = False)\n",
    "            feed_dict={seq: epoch_x, targets: epoch_y, train_labels:label_ind, num_samples: n_samples, output_indexes: out_ind, context_future:fut_context, merchant_labels : m_ind, publisher_labels : p_ind, history_labels: h_ind, no_history_labels: nh_ind}\n",
    "            p_recs, m_recs = sess.run([  top_n_pubs, top_n_merchants ], feed_dict)\n",
    "            p_recs = p_recs.reshape(1,-1).tolist()\n",
    "            m_recs = m_recs.reshape(1,-1).tolist()\n",
    "            p_rec_list = p_rec_list + [mapping_reverse[item+len(this_merchants)]  for item in p_recs[0]]\n",
    "            m_rec_list = m_rec_list +[mapping_reverse[item]  for item in m_recs[0]]\n",
    "        return np.asarray(p_rec_list).reshape(-1, recall_num), np.asarray(m_rec_list).reshape(-1, recall_num)\n",
    "\n",
    "# read in the dictionary which is the embedding from spark\n",
    "with open(r\"/home/alex/Skimlinks/kw_data/one_week_embeddings.pkl\", \"rb\") as input_file:\n",
    "    dictionary = pickle.load(input_file)\n",
    "\n",
    "embedding_list = dictionary.values()\n",
    "domain_ids = dictionary.keys()\n",
    "dictionary['0'] = [0.0]*50\n",
    "\n",
    "with open(r\"/home/alex/Skimlinks/kw_data/kw.pkl\", \"rb\") as input_file:\n",
    "    kw = pickle.load(input_file)\n",
    "\n",
    "with open(r\"/home/alex/Skimlinks/kw_data/merchants.pkl\", \"rb\") as input_file:\n",
    "    merchants = pickle.load(input_file)\n",
    "\n",
    "def append_keyword(ids, this_kws, lst_domain_kw):\n",
    "    for item in set(this_kws):\n",
    "        lst_domain_kw.append(list((ids,item))) \n",
    "    if key_words == []:\n",
    "        lst_domain_kw.append(list((ids,\"\")))\n",
    "\n",
    "merchant_set = set(merchants)\n",
    "\n",
    "start = time.time()\n",
    "with open('/home/alex/Skimlinks/kw_data/sessions.txt', 'rt') as f:\n",
    "    reader= csv.reader(f, delimiter=',')\n",
    "    data, d_kw = [], []\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        this_row = literal_eval(','.join(row))\n",
    "        key_words = []\n",
    "        session_domains = [x[1] for x in this_row]\n",
    "        prev_did = str(session_domains[0])\n",
    "        \n",
    "        # get sessions with at least 2 different domains\n",
    "        if len(set(session_domains))>1:\n",
    "            tmp, dt, ts = [], [], []\n",
    "            for elm in this_row:\n",
    "                did = str(elm[1])\n",
    "                if did == prev_did:\n",
    "                    key_words.extend(elm[2])\n",
    "                    dt.append(np.log2(elm[-1]))\n",
    "                    ts.append(elm[0]/3600 % 24)\n",
    "                    \n",
    "                else:\n",
    "                    if prev_did in merchant_set:\n",
    "                        m=1\n",
    "                    else: m = 0\n",
    "                    tmp.append(list((prev_did,key_words,np.round(np.mean(dt)).tolist(), ts[-1], elm[-2],m)))\n",
    "                    append_keyword(prev_did, key_words, d_kw)\n",
    "                        \n",
    "                    key_words, dt, ts = [], [], []\n",
    "                    dt.append(np.log2(elm[-1]))\n",
    "                    key_words.extend(elm[2])\n",
    "                    ts.append(elm[0]/3600 % 24)\n",
    "                    prev_did = did\n",
    "                \n",
    "                if elm == this_row[-1]: \n",
    "                    if prev_did in merchant_set:\n",
    "                        m=1\n",
    "                    else: m =0\n",
    "                    tmp.append(list((prev_did,key_words,np.round(np.mean(dt)).tolist(), ts[-1], elm[-2],m)))\n",
    "                    append_keyword(prev_did, set(key_words), d_kw)\n",
    "            data.append(tmp)\n",
    "        count+=1\n",
    "        if count % 100000 == 0:\n",
    "            print(\"processing row :{}, time taken so far:{}\".format(count, time.time() - start))\n",
    "            \n",
    "print(time.time()- start)              \n",
    "# ID, kw, date, dt        \n",
    "    \n",
    "df = pd.DataFrame(d_kw)\n",
    "df.columns = ['domain', 'kw']\n",
    "gb = df.groupby(['domain'])\n",
    "domain_kw = {k: set(v) for k,v in gb[\"kw\"]}\n",
    "\n",
    "\n",
    "size_dt = 20\n",
    "size_id = 2\n",
    "size_hour = 24\n",
    "size_day = 31 \n",
    "recall_number = 10\n",
    "\n",
    "all_domains = gb.groups.keys()\n",
    "\n",
    "# get the domain ids since I forgot to earlier\n",
    "domains = set(all_domains)\n",
    "merchant_set = set(merchants)\n",
    "this_merchants = set.intersection(merchant_set,domains)\n",
    "pub_ids = domains - this_merchants\n",
    "this_merchants = list(this_merchants)\n",
    "pub_ids = list(pub_ids)\n",
    "pub_ids = [str(x) for x in pub_ids]\n",
    "\n",
    "av_kw_domain_embed = {}\n",
    "for elm in domain_kw.keys():\n",
    "    current_kw = domain_kw[elm]\n",
    "    current_embed = average_keywords_domains(current_kw, kw)\n",
    "    av_kw_domain_embed[elm] = current_embed\n",
    "\n",
    "# get the average merchants\n",
    "prior = np.reshape(np.mean(np.asarray(merchant_average(this_merchants, dictionary)),axis = 0),(-1,1))\n",
    "\n",
    "ids = this_merchants+ pub_ids\n",
    "indx = range(len(this_merchants)+ len(pub_ids))\n",
    "# Getting a one hot encoding by indexing each id.\n",
    "mapping = dict(zip(ids,indx))\n",
    "mapping_reverse = dict(zip(indx, ids))\n",
    "\n",
    "KEY_WORDS = True\n",
    "embed_size = 50\n",
    "embed_future = 300 +size_dt+ size_id+size_hour+ size_day# chosen prior\n",
    "if KEY_WORDS == True:\n",
    "    embed_size = 50 + 300 \n",
    "    \n",
    "# define parameters\n",
    "rnn_size = 100 # hidden layer size\n",
    "output_size = len(ids) # output after rnn\n",
    "max_length = 19 # max seq length is 20, last is taken as target\n",
    "learning_rate = 0.0015\n",
    "batch_size = 128\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# placeholders tp store the inputs and labels \n",
    "seq = tf.placeholder(tf.float32, [None, max_length ,embed_size],name='seq')\n",
    "\n",
    "targets = tf.placeholder(tf.float32,shape  = [None,None],name='LabelData')\n",
    "\n",
    "train_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "\n",
    "output_indexes = tf.placeholder(tf.int32, shape=[None,1], name = 'indexes_data')\n",
    "\n",
    "num_samples = tf.placeholder(tf.int32, shape = [],name = \"Negative_sample_num\")\n",
    "\n",
    "context_future = tf.placeholder(tf.float32,shape  = [None,embed_future],name='future_context')\n",
    "\n",
    "merchant_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "\n",
    "publisher_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "\n",
    "history_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "\n",
    "no_history_labels = tf.placeholder(tf.int32, shape=[None,1])\n",
    "\n",
    "variables = {'weights':tf.Variable(tf.random_normal([rnn_size,output_size],stddev= 0.1),name='Weights1'),\n",
    "             'biases':tf.Variable(tf.random_normal([output_size],stddev = 0.1),name='Bias')}\n",
    "    \n",
    "    \n",
    "# Here the gru cell is defined of specified size\n",
    "gru_cell = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "\n",
    "# The ouputs are a tensor of all the ouput states of the pixels\n",
    "outputs, states = tf.nn.dynamic_rnn(cell = gru_cell, inputs = seq, dtype=tf.float32, sequence_length=lengths(seq)[0],scope = 't')\n",
    "\n",
    "\n",
    "#last = last_relevant(outputs, length(seq)[0])\n",
    "last = get_relevant(outputs,output_indexes)\n",
    "\n",
    "\n",
    "output = tf.matmul(last,variables['weights'])+variables['biases']\n",
    "\n",
    "top_n, top_n_index =  tf.nn.top_k(output,num_samples+1)\n",
    "\n",
    "# basically we end up repeating the target score before the softmax across the cols, \n",
    "# so that we can use it tp subtract later\n",
    "cat_idx = tf.concat([tf.reshape(tf.range(0, tf.shape(output)[0]), [-1,1]), train_labels], axis=1, name = \"Targets\")\n",
    "\n",
    "target_ind = tf.reshape(tf.gather_nd(output, cat_idx),[-1,1])\n",
    "\n",
    "# Tensor(matrix) with repeated target values repated for the number of negative samples \n",
    "target_ind = tf.tile(target_ind, tf.stack([1, num_samples]))\n",
    "\n",
    "top_neg_vals, top_neg_indice =  tf.nn.top_k(output,num_samples)\n",
    "\n",
    "top_5_indx = tf.slice(top_neg_indice,[0,0],[-1,recall_number])\n",
    "#top_5_indx = tf.nn.top_k(output,recall_number).indices\n",
    "# bpr max loss\n",
    "\n",
    "diff =  target_ind - top_neg_vals\n",
    "\n",
    "activate = tf.sigmoid(diff)\n",
    "\n",
    "# need to check if the softmax is with resoect to all or just sampled\n",
    "softmax_scores = tf.nn.softmax(top_neg_vals)\n",
    "\n",
    "lmbda = 1\n",
    "regularizer = softmax_scores*tf.square(top_neg_vals)\n",
    "loss = activate * softmax_scores\n",
    "\n",
    "with tf.name_scope('bpr_Loss'):\n",
    "    loss = tf.reduce_mean(-tf.log(tf.reduce_sum(loss,1))+ lmbda* tf.reduce_sum(regularizer,1))\n",
    "# Xent =  tf.nn.softmax_cross_entropy_with_logits(logits = output, labels = targets)\n",
    "# with tf.name_scope('bpr_Loss'):\n",
    "#      loss = tf.reduce_mean(Xent)\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_label = tf.equal(tf.argmax(output, 1), tf.argmax(targets, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_label, tf.float32))\n",
    "\n",
    "with tf.name_scope('Mean_Reciprical_rank'):\n",
    "    ordered_ranks = tf.cast(tf.nn.top_k(output,len(ids)).indices, tf.float32, name = 'take_top')\n",
    "    mrr_labels = tf.cast(tf.tile(train_labels, [1, len(ids)]),tf.float32)\n",
    "    ranks = tf.cast(tf.argmax(tf.cast(tf.equal(mrr_labels, ordered_ranks), tf.float32),1) +1, tf.float32)\n",
    "    recip_rank = tf.reduce_mean(tf.reciprocal(ranks))\n",
    "\n",
    "    \n",
    "\n",
    "repeated_labels = tf.tile(train_labels, [1, recall_number])  # Create multiple columns.\n",
    "correct_recall = tf.equal(repeated_labels, top_5_indx)\n",
    "correct_recall = tf.cast(correct_recall, tf.float32)\n",
    "recall_hits = tf.reduce_sum(correct_recall, 1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Recall\"):\n",
    "    final_recall = tf.reduce_mean(tf.reduce_sum(correct_recall, 1))\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"Merchant_Recall\"):\n",
    "    M_correct_recall = tf.reshape(tf.gather(correct_recall, merchant_labels),[-1, recall_number])\n",
    "    M_final_recall = tf.reduce_mean(tf.reduce_sum(M_correct_recall, 1))\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"Publisher_Recall\"):\n",
    "    P_correct_recall = tf.reshape(tf.gather(correct_recall, publisher_labels),[-1, recall_number])\n",
    "    P_final_recall = tf.reduce_mean(tf.reduce_sum(P_correct_recall, 1))\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"History_Recall\"):\n",
    "    H_correct_recall = tf.reshape(tf.gather(correct_recall, history_labels),[-1, recall_number])\n",
    "    H_final_recall = tf.reduce_mean(tf.reduce_sum(H_correct_recall, 1))\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"NoHistory_Recall\"):\n",
    "    NH_correct_recall = tf.reshape(tf.gather(correct_recall, no_history_labels),[-1, recall_number])\n",
    "    NH_final_recall = tf.reduce_mean(tf.reduce_sum(NH_correct_recall, 1))\n",
    "# return merchant recommedations during testing\n",
    "top_n_merchants = tf.nn.top_k(tf.slice(output,[0,0],[-1,len(this_merchants)]),recall_number).indices\n",
    "top_n_pubs = tf.nn.top_k(tf.slice(output,[0,len(this_merchants)],[-1,len(pub_ids)]),recall_number).indices\n",
    "\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss_pbpr\", loss)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "tf.summary.scalar(\"Publisher_Recall\", P_final_recall)\n",
    "tf.summary.scalar(\"Merchant_Recall\", M_final_recall)\n",
    "tf.summary.scalar(\"History_Recall\", H_final_recall)\n",
    "tf.summary.scalar(\"NoHistory_Recall\", NH_final_recall)\n",
    "tf.summary.scalar(\"Recall\", final_recall)\n",
    "tf.summary.scalar(\"Mean_Reciprical_rank\", recip_rank)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "\n",
    "logs_path = '/tmp/tensorflow_logs/key_words_context'\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "# Need to save the model, weights and biases varibles\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "\n",
    "# Suggested Directory to use\n",
    "save_MDir = 'models/'\n",
    "\n",
    "\n",
    "#create the directory if it does not exist already\n",
    "if not os.path.exists(save_MDir):\n",
    "    os.makedirs(save_MDir)\n",
    "\n",
    "save_model = os.path.join(save_MDir,'model_context_features')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "n_samples = len(ids)//3\n",
    "restoreModel = False\n",
    "if restoreModel == False:\n",
    "    accs,recalls, mrrs, r_m, r_p, r_h, r_nh  = optimize(data[:-100000], dictionary, 128, 5, max_length, embed_size, learning_rate, data[-100000:])\n",
    "else:    \n",
    "    # name of directory to read model from\n",
    "    save_MDir = 'models/'\n",
    "    save_model = os.path.join(save_MDir,'model_context_features') \n",
    "    pub_recomendations, merchant_recommendations = reommend_pub_merchants(data[-10000:], dictionary, 128, max_length, embed_size, recall_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
